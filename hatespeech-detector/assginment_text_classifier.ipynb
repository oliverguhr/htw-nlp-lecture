{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text \n",
    "\n",
    "In this little turorial we are using PyTorch, TorchText and Byte Pair Encoding to quickly build a text classifyer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install bpemb pandas torchtext torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bpemb import BPEmb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data\n",
    "\n",
    "\n",
    "At first, we need to downlad the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.htw-dresden.de/~guhr/dist/sample/germeval2018.training.txt\n",
    "!wget https://www.htw-dresden.de/~guhr/dist/sample/germeval2018.test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data, using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"germeval2018.test.txt\", sep='\\t', header=0,encoding=\"utf-8\")\n",
    "train_df = pd.read_csv(\"germeval2018.training.txt\", sep='\\t', header=0,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "test_df.drop(columns=['label2'], inplace=True)\n",
    "train_df.drop(columns=['label2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "Now we can preprocess our dataset. In this step we remove all special chars and binarize our labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9öäüÖÄÜß()!?]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text\n",
    "\n",
    "def convert_label(label):\n",
    "    return 1 if label == \"OFFENSE\" else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"]=clean_text(train_df[\"text\"])\n",
    "test_df[\"text\"]=clean_text(test_df[\"text\"])\n",
    "train_df[\"label\"]=train_df[\"label\"].map(convert_label)\n",
    "test_df[\"label\"]=test_df[\"label\"].map(convert_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is  how our data set looks now. No urls no @ :)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following will help make the results reproducible later.\n",
    "# This is will make shure that you get the same result every time you train you model\n",
    "# Turn this off, for you final train run, to improve performance.\n",
    "SEED = 42\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data magic\n",
    "\n",
    "The following class helps us to convert the pandas dataframe into an pytorch data set. You can skip that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://gist.github.com/lextoumbourou/8f90313cbc3598ffbabeeaa1741a11c8\n",
    "# to use DataFrame as a Data source\n",
    "\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, is_test=False, **kwargs):\n",
    "        print(df)\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():            \n",
    "            label = row.label#row.target if not is_test else None            \n",
    "            text = row.text            \n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df, data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df, data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df, data_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the pretrained word vectors\n",
    "\n",
    "For this tutorial we are using the byte pair encoding. The great [BPEmb](https://pypi.org/project/bpemb/) library helps us the encode the text and provides pretrained models for a lot of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from torchtext import vocab\n",
    "\n",
    "bpemb_de = BPEmb(lang=\"de\", vs=10000)\n",
    "bpemb_de_counter = Counter(bpemb_de.words)\n",
    "bpemb_de_stoi = {word:i for i, word in enumerate(bpemb_de.words)}\n",
    "\n",
    "bpemb_vocab = vocab.Vocab(counter = bpemb_de_counter)\n",
    "bpemb_vocab.set_vectors(stoi = bpemb_de_stoi, vectors = torch.tensor(bpemb_de.vectors), dim = bpemb_de.dim)\n",
    "\n",
    "bpemb_vocab.stoi = bpemb_de_stoi # pytorch overwrite our tokens, so we need to reset them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The byte pair encoding turns words into tokens. Every tokens has an id and a coresponding vector that we can feed to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bpemb_de.encode_with_bos_eos(\"das ist ein test\")\n",
    "print(tokens)\n",
    "\n",
    "token_ids = bpemb_de.encode_ids_with_bos_eos(\"das ist ein test\")\n",
    "print(token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this is how the vector for the \"_das\" token looks like:\n",
    "bpemb_de.vectors[99]\n",
    "#[bpemb_de.vectors[id] for id in token_ids] # vectors for all tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Train and Valid Data Sets\n",
    "\n",
    "First, we define how the TEXT and LABEL's will encoded. Thats what the Field fields do. With these fields and the class we defined above we can create a data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize= bpemb_de.encode,init_token ='<s>', eos_token='</s>',pad_token=\"<unk>\",use_vocab = True, batch_first = True,sequential=True )\n",
    "\n",
    "TEXT.vocab = bpemb_vocab # -> assign our byte pair endcoing module\n",
    "LABEL = data.LabelField(dtype = torch.float, use_vocab = False)\n",
    "\n",
    "fields = [('text',TEXT), ('label',LABEL)]\n",
    "train_ds, val_ds = DataFrameDataset.splits(fields, train_df=train_df, val_df=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at a the first example\n",
    "print(vars(train_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Iterator\n",
    "\n",
    "With this data set we can now create a iterator that prepares the batches for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, valid_iterator = data.Iterator.splits(\n",
    "    (train_ds, val_ds), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,    \n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how a batch looks like. Do you know why our texts a still id's?\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "\n",
    "print(batch.label)\n",
    "print(batch.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the Model\n",
    "\n",
    "Now its finally time to define our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, weights,embedding_length = 100):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        # these three lines load to pretrained vecotrs into our embedding layer\n",
    "        vocab_size= len(weights)        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length) \n",
    "        self.word_embeddings.weight = nn.Parameter(weights, requires_grad=False)                \n",
    "        \n",
    "    def forward(self, input_sentences):\n",
    "        input = self.word_embeddings(input_sentences) # <-- here we turn our ids into actual vectors\n",
    "        \n",
    "        # since our sentences are do not have a equal length, we can't simply feed them \n",
    "        # into a feed forward network. How can we solve that?\n",
    "        \n",
    "        return input # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model\n",
    "\n",
    "First we define a set of helper funtions, to make our live a bit easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we moved the training of a single batch into a method for convenience\n",
    "def train(model, iterator):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text = batch.text\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...same with the eval code\n",
    "def evaluate(model, iterator):\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch.text\n",
    "            predictions = model(text).squeeze(1)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can create an instance of our model, with the pretrained byte pair vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel(torch.tensor(bpemb_de.vectors))\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator)\n",
    "    valid_acc = evaluate(model, valid_iterator)\n",
    "    \n",
    "    print(f'{epoch} Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Acc: {valid_acc*100:.2f}%')    \n",
    "    \n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1,num_epochs+1)\n",
    "#plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='validation acc')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1. Implement a feed forward neural entwork classifyer\n",
    "\n",
    "2. Try to improve the results. What happens when,\n",
    "    * you use more layers\n",
    "    * more neurons\n",
    "    * a bigger vocabulary size\n",
    "    \n",
    "3. Try differnt models:\n",
    "    * Use LSTMs \n",
    "    * Did you know that you can use a cnn to classify text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
